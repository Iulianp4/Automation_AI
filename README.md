# Automation_AI ‚Äî Test Case Generator

Generate high-quality UI test cases from **Requirements / Acceptance Criteria / Use Cases**, add tester **Details**, export execution-ready Excel, and **compare** AI output vs. a **manual baseline** (precision/recall/F1, coverage, distributions, traceability).

---

## ‚ú® Features

- **Input sources**: `requirements.xlsx`, `acceptance_criteria.xlsx`, `use_cases.xlsx` (+ optional `manual_cases.xlsx`)
- **Extra tester context** via `*_details` to boost coverage
- **Balanced test mix** (Positive / Negative / Boundary / Security / AdHoc)
- **Output styles**: classic steps or Given/When/Then (Gherkin) ‚Äî or both
- **Exports**: `generated_raw`, `execution_export`, `traceability`, `metrics`, `legend`, `run_info`
- **Comparison** vs manual: greedy matching + **precision / recall / F1**, per-category & per-requirement stats, trace matrix
- **Comparison-only mode** (skip generation; reuse `results/*.xlsx`)
- **Robust OpenAI calls**: retry, timeout, token usage print

---

## üóÇ Repo structure

```
Automation_AI/
‚îú‚îÄ data/
‚îÇ  ‚îú‚îÄ requirements.xlsx
‚îÇ  ‚îú‚îÄ acceptance_criteria.xlsx
‚îÇ  ‚îú‚îÄ use_cases.xlsx
‚îÇ  ‚îî‚îÄ manual_cases.xlsx        # optional baseline
‚îú‚îÄ results/
‚îÇ  ‚îú‚îÄ report_from_requirements.xlsx
‚îÇ  ‚îú‚îÄ report_from_acceptance.xlsx
‚îÇ  ‚îú‚îÄ report_from_use_cases.xlsx
‚îÇ  ‚îú‚îÄ report.xlsx              # consolidated AI
‚îÇ  ‚îî‚îÄ report_comparison.xlsx   # AI vs Manual
‚îú‚îÄ src/
‚îÇ  ‚îú‚îÄ config.py
‚îÇ  ‚îú‚îÄ preprocess.py
‚îÇ  ‚îú‚îÄ generate_gpt.py
‚îÇ  ‚îú‚îÄ comparison.py
‚îÇ  ‚îî‚îÄ __init__.py
‚îú‚îÄ main.py
‚îú‚îÄ .env                        # contains OPENAI_API_KEY (NOT committed)
‚îú‚îÄ README.md
‚îú‚îÄ CHANGELOG.md
‚îî‚îÄ requirements.txt
```

---

## üîß Setup

1. **Python & venv**
   ```bash
   py -m venv venv
   venv\Scripts\activate
   pip install -r requirements.txt
   ```

2. **Environment**
   Create `.env` (do NOT commit):
   ```
   OPENAI_API_KEY=sk-...
   # optional:
   OPENAI_MODEL=gpt-4o-mini
   ```

3. **Config flags** (`src/config.py`)
   ```python
   INTERACTIVE = True  # set False to run with defaults (no prompts)
   DEFAULTS = {
     "num_req": 5, "num_ac": 5, "num_uc": 5,
     "output_style": "both",              # classic / gherkin / both
     "include_ad_hoc": True,
     "mix": "balanced",                   # balanced / positive_heavy / negative_heavy
     "compare_only": False,
     "do_comparison": True,
     "similarity_strategy": "title_expected",  # or title_steps_expected
     "similarity_threshold": 0.75,
   }
   ```

---

## üì• Data templates (columns)

- **requirements.xlsx**
  - `requirement_id`, `requirement_name`, `requirement_text`,
    `requirement_rationale` (opt), `requirement_platform` (opt),
    `requirement_details` (extra tester context)

- **acceptance_criteria.xlsx**
  - `acceptance_criteria_story_id` ‚Üí `requirement_id`
  - `acceptance_criteria` ‚Üí `ac_text`
  - `acceptance_criteria_notes` (opt), `acceptance_criteria_comments` (opt)
  - `acceptance_criteria_details` ‚Üí `ac_details`

- **use_cases.xlsx**
  - `use_cases_story_id` ‚Üí `requirement_id`
  - Any of: `use_cases_title`, `use_cases_description`,
    `use_cases_preconditions`, `use_cases_main_flow`,
    `use_cases_alternative_flows`, `use_cases_exception_flows`,
    `use_cases_business_rules`
  - `use_cases_details` ‚Üí `uc_details` (extra tester context)

- **manual_cases.xlsx** (optional baseline; friendly headers accepted)
  - `Requirement ID, Requirement Name, Test Case ID, Title, Preconditions, Steps,
     Test Data, Expected Result, Category, Gherkin, Source`

> Missing IDs fall back to synthetic: `REQ-LONE-*`, `AC-LONE-*`, `UC-LONE-*`.

---

## ‚ñ∂Ô∏è Run

### Generate + (optional) compare
```bash
python main.py
```
- Choose **how many tests per source** (Req/AC/UC), **style** (classic/gherkin/both),
  **mix** (balanced/positive_heavy/negative_heavy), allow **AdHoc** or not.
- Results saved to `results/report_from_*.xlsx` and `results/report.xlsx`.

If `data/manual_cases.xlsx` exists, you‚Äôll be prompted to **compare AI vs manual** and produce `results/report_comparison.xlsx`.

### Comparison-only mode
Use existing AI results from `results/*.xlsx` (sheet `generated_raw`) without regenerating:
- When prompted: *Run comparison only (skip generation)?* ‚Üí **yes**, or
- Set in `config.py`: `INTERACTIVE=False`, `DEFAULTS["compare_only"]=True`.

---

## üì§ Outputs (Excel sheets)

- **generated_raw** ‚Äì normalized test cases
- **execution_export** ‚Äì `Nr.Crt / Steps / Actual Result / Expected Result / Document of evidence`
- **traceability** ‚Äì one row / test (with req id/name, category, has gherkin)
- **metrics** ‚Äì totals, by source, by category, by requirement, cat√ósource pivot
- **legend** ‚Äì input/output field dictionary
- **run_info** ‚Äì parameters used

**Comparison (`report_comparison.xlsx`):**
- `matches` ‚Äì aligned pairs with similarity
- `ai_only`, `manual_only` ‚Äì non-matched items
- `scores_summary` ‚Äì overall **precision, recall, F1, grade**
- `dist_by_category` ‚Äì AI vs manual (counts & %)
- `per_requirement_density` ‚Äì counts by requirement id
- `trace_matrix` ‚Äì cross-tab of matched reqs (AI√óManual)

---

## üß† How it works (short)

- `preprocess.py` reads & normalizes Excel, groups AC/UC by `requirement_id`,
  builds per-row context + optional `*_details`.
- `generate_gpt.py` builds a strict JSON prompt, **distributes categories** based on your `mix`, calls OpenAI with **retry/timeout** and prints token usage.
- `main.py` orchestrates generation/export for each source and the consolidated report; supports **comparison-only**.  
- `comparison.py` computes greedy matches and metrics (**precision/recall/F1**, coverage, novelty, quality), plus analysis tables.

---

## ü©π Troubleshooting

- **PowerShell cannot activate venv**  
  `Set-ExecutionPolicy -Scope CurrentUser RemoteSigned` (then retry).
- **Missing `openpyxl`**  
  `pip install openpyxl`
- **API key / quota**  
  Ensure `.env` contains `OPENAI_API_KEY` and your account has credit.
- **Invalid columns**  
  The app prints `‚ö†Ô∏è <file>: missing columns -> ...`. Fix Excel headers accordingly.
- **No matches in comparison**  
  Lower `DEFAULTS["similarity_threshold"]` (e.g., `0.65`) or use `title_steps_expected` strategy.

---

## üîí Security & privacy

- **Never commit secrets** (`.env` in `.gitignore`).
- Use sanitized, non-confidential data for public demos.
- For company data, confirm policy & retention before usage.

---

## üìÑ License

MIT (or your preferred license). Update `LICENSE` accordingly.
